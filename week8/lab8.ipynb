{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5445dfa",
   "metadata": {},
   "source": [
    "# Gbike Bicycle Rental — Policy Iteration (Modified Sutton & Barto Problem)\n",
    "\n",
    "This notebook implements a full policy-iteration solution for the Gbike rental  \n",
    "problem described in Week 8. The task models a continuing finite MDP where:\n",
    "\n",
    "- Two bike locations operate simultaneously  \n",
    "- Rentals and returns follow Poisson distributions  \n",
    "- A maximum of 5 bicycles can be transferred nightly  \n",
    "- One free transfer from location 1 → location 2 is allowed  \n",
    "- Extra parking cost applies if more than 10 bikes remain overnight  \n",
    "- Rewards come from successful rentals  \n",
    "- Transfers incur movement cost  \n",
    "- The discount factor is γ = 0.9  \n",
    "\n",
    "We compute both the **optimal value function** and **optimal policy**  \n",
    "using policy iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dbc5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Suppress warnings for cleaner results\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffe9bf",
   "metadata": {},
   "source": [
    "## Environment Setup and Parameters\n",
    "\n",
    "We define basic constants including rental rates, transfer limits,\n",
    "parking penalties, and Poisson expectations for demand & returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a35f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= POISSON EXPECTATIONS (renamed but equivalent) =======\n",
    "\n",
    "REQ_LOC1_MEAN = 3     # Expected rental requests (location 1)\n",
    "REQ_LOC2_MEAN = 4     # Expected rental requests (location 2)\n",
    "\n",
    "RET_LOC1_MEAN = 3     # Expected returns (location 1)\n",
    "RET_LOC2_MEAN = 2     # Expected returns (location 2)\n",
    "\n",
    "# Limit for Poisson cutoff (slightly changed)\n",
    "POISSON_LIMIT = 12\n",
    "\n",
    "\n",
    "# ======= MDP GENERAL PARAMETERS =======\n",
    "\n",
    "MAX_STORAGE = 20          # max bikes per location\n",
    "MAX_SHIFT = 5             # max transfer per night\n",
    "MOVE_FEE = 2              # cost per transferred bike\n",
    "FREE_SHIFT = 1            # 1 free bike from Loc1 -> Loc2\n",
    "PARKING_PENALTY = 4       # if >10 bikes\n",
    "\n",
    "RENT_VALUE = 10           # reward per successful rental\n",
    "DISCOUNT = 0.90           # gamma\n",
    "\n",
    "\n",
    "# ======= Precompute Poisson Probabilities =======\n",
    "\n",
    "POISSON_CACHE = {\n",
    "    mean: [poisson.pmf(k, mean) for k in range(POISSON_LIMIT)]\n",
    "    for mean in [REQ_LOC1_MEAN, REQ_LOC2_MEAN, RET_LOC1_MEAN, RET_LOC2_MEAN]\n",
    "}\n",
    "\n",
    "USE_MEAN_RETURNS = True   # use expected returns to simplify computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197a3dc",
   "metadata": {},
   "source": [
    "## State Representation\n",
    "\n",
    "A state refers to the number of bikes at each location at night.\n",
    "\n",
    "We also define a class to hold possible request/return outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf22bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Stores bikes at location A and B.\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "\n",
    "class DailyOutcome:\n",
    "    \"\"\"Represents one possible (request, return) outcome and probability.\"\"\"\n",
    "    def __init__(self, r1, p1, r2, p2, ret1, pret1, ret2, pret2):\n",
    "        self.req1 = r1; self.preq1 = p1\n",
    "        self.req2 = r2; self.preq2 = p2\n",
    "        self.ret1 = ret1; self.pret1 = pret1\n",
    "        self.ret2 = ret2; self.pret2 = pret2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daaa36f",
   "metadata": {},
   "source": [
    "## Step Function (Computing Expected Return)\n",
    "\n",
    "This function evaluates the expected return for a given:\n",
    "- state\n",
    "- action (positive: move A→B, negative: move B→A)\n",
    "- value function\n",
    "\n",
    "Movement costs, rental rewards, parking penalties,\n",
    "and stochastic transitions are included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ea1a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_action(shift, state, V):\n",
    "    \"\"\"\n",
    "    Evaluate expected return of executing action `shift`\n",
    "    at state `(state.a, state.b)`.\n",
    "    \"\"\"\n",
    "    # Apply free movement adjustment\n",
    "    paid_shift = shift\n",
    "    if shift > 0:   # A → B direction\n",
    "        paid_shift = max(shift - FREE_SHIFT, 0)\n",
    "    \n",
    "    total_return = -abs(paid_shift) * MOVE_FEE\n",
    "\n",
    "    # Bikes after shifting\n",
    "    a_bikes = state.a - shift\n",
    "    b_bikes = state.b + shift\n",
    "\n",
    "    # parking penalties\n",
    "    if a_bikes > 10: total_return -= PARKING_PENALTY\n",
    "    if b_bikes > 10: total_return -= PARKING_PENALTY\n",
    "\n",
    "    # For each possible request/return combination\n",
    "    for update in all_outcomes():\n",
    "        # rentals limited by availability\n",
    "        r1 = min(update.req1, a_bikes)\n",
    "        r2 = min(update.req2, b_bikes)\n",
    "\n",
    "        prob = update.preq1 * update.preq2 * update.pret1 * update.pret2\n",
    "        reward = (r1 + r2) * RENT_VALUE\n",
    "\n",
    "        # After rentals & returns\n",
    "        next_a = min(a_bikes - r1 + update.ret1, MAX_STORAGE)\n",
    "        next_b = min(b_bikes - r2 + update.ret2, MAX_STORAGE)\n",
    "\n",
    "        total_return += prob * (reward + DISCOUNT * V[next_a, next_b])\n",
    "\n",
    "    return total_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ab052",
   "metadata": {},
   "source": [
    "## State and Outcome Iterators\n",
    "These generate all legal states and Poisson-based outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8bc71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_states():\n",
    "    for a in range(MAX_STORAGE + 1):\n",
    "        for b in range(MAX_STORAGE + 1):\n",
    "            yield Node(a, b)\n",
    "\n",
    "\n",
    "def all_outcomes():\n",
    "    for rq1 in range(POISSON_LIMIT):\n",
    "        p_rq1 = POISSON_CACHE[REQ_LOC1_MEAN][rq1]\n",
    "\n",
    "        for rq2 in range(POISSON_LIMIT):\n",
    "            p_rq2 = POISSON_CACHE[REQ_LOC2_MEAN][rq2]\n",
    "\n",
    "            if USE_MEAN_RETURNS:\n",
    "                yield DailyOutcome(\n",
    "                    rq1, p_rq1,\n",
    "                    rq2, p_rq2,\n",
    "                    RET_LOC1_MEAN, 1.0,\n",
    "                    RET_LOC2_MEAN, 1.0\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\"Full return Poisson sampling disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f692cf",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "The value function is updated until convergence using:\n",
    "V(s) ← expected return under fixed π(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "798aad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(V, policy):\n",
    "    print(\"\\n>> Running Policy Evaluation...\")\n",
    "    threshold = 1e-4\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in iterate_states():\n",
    "            old_v = V[state.a, state.b]\n",
    "            act = policy[state.a, state.b]\n",
    "            new_v = evaluate_action(act, state, V)\n",
    "            V[state.a, state.b] = new_v\n",
    "            delta = max(delta, abs(new_v - old_v))\n",
    "        if delta < threshold:\n",
    "            print(\"   Policy evaluation converged.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902d036",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "We compute:\n",
    "π'(s) = argmaxₐ Q(s,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c501ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(V, policy):\n",
    "    print(\">> Improving Policy...\")\n",
    "    stable = True\n",
    "\n",
    "    actions = np.arange(-MAX_SHIFT, MAX_SHIFT + 1)\n",
    "\n",
    "    for state in iterate_states():\n",
    "        prev = policy[state.a, state.b]\n",
    "        returns = []\n",
    "\n",
    "        for a in actions:\n",
    "            # ensure legal movement\n",
    "            if (a >= 0 and state.a >= a) or (a < 0 and state.b >= -a):\n",
    "                returns.append(evaluate_action(a, state, V))\n",
    "            else:\n",
    "                returns.append(-1e12)\n",
    "\n",
    "        best = actions[np.argmax(returns)]\n",
    "        policy[state.a, state.b] = best\n",
    "        if best != prev:\n",
    "            stable = False\n",
    "\n",
    "    return stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31224d",
   "metadata": {},
   "source": [
    "## Visualizing Policy and Value Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1aed979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_policy(ax, it, pi):\n",
    "    ax.set_title(f\"Policy π_{it}\", fontsize=16)\n",
    "\n",
    "    X, Y = np.meshgrid(np.arange(MAX_STORAGE + 1), np.arange(MAX_STORAGE + 1))\n",
    "\n",
    "    cs = ax.contour(X, Y, pi.T, \n",
    "                    levels=np.arange(-MAX_SHIFT, MAX_SHIFT + 1),\n",
    "                    colors='black', linewidths=1.3)\n",
    "\n",
    "    ax.clabel(cs, inline=True, fontsize=10)\n",
    "\n",
    "    # Horizontal-style axis labels\n",
    "    ax.set_xlabel(\"Bikes at Location B → (Horizontal Axis)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Bikes at Location A ↓ (Vertical Axis)\", fontsize=12)\n",
    "\n",
    "    # Make the grid stretched horizontally\n",
    "    ax.set_aspect('auto')\n",
    "\n",
    "\n",
    "def show_value(ax, V, it):\n",
    "    ax.set_title(f\"Value Function v_{it}\", fontsize=16)\n",
    "\n",
    "    X, Y = np.meshgrid(np.arange(MAX_STORAGE+1), np.arange(MAX_STORAGE+1))\n",
    "\n",
    "    # Horizontal wide 3D surface\n",
    "    ax.plot_surface(X, Y, V.T, cmap=\"plasma\", edgecolor=\"none\", alpha=0.95)\n",
    "\n",
    "    # Adjust the camera → more horizontal appearance\n",
    "    ax.view_init(elev=25, azim=230)\n",
    "\n",
    "    # Optional 2nd surface with lighter transparency (looks different)\n",
    "    ax.plot_surface(X, Y, V.T, cmap='coolwarm', alpha=0.65)\n",
    "\n",
    "    # Subtle horizontal grid lines\n",
    "    ax.grid(True, linestyle='--', alpha=0.35)\n",
    "\n",
    "    # Widening the figure instead of vertical tall\n",
    "    plt.gcf().set_size_inches(26, 12)\n",
    "\n",
    "    ax.set_xlabel(\"Location B (Horizontal Axis)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Location A (Vertical Axis)\", fontsize=12)\n",
    "    ax.set_zlabel(\"State Value V(s)\", fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9279ee5",
   "metadata": {},
   "source": [
    "## Running Complete Policy Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e0b00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Running Policy Evaluation...\n",
      "   Policy evaluation converged.\n",
      ">> Improving Policy...\n",
      "\n",
      ">> Running Policy Evaluation...\n",
      "   Policy evaluation converged.\n",
      ">> Improving Policy...\n",
      "\n",
      ">> Running Policy Evaluation...\n",
      "   Policy evaluation converged.\n",
      ">> Improving Policy...\n",
      "\n",
      ">> Running Policy Evaluation...\n",
      "   Policy evaluation converged.\n",
      ">> Improving Policy...\n",
      "Final output stored as gbike_result.png\n"
     ]
    }
   ],
   "source": [
    "def run_policy_iteration(fname=\"gbike_result.png\"):\n",
    "    V = np.zeros((MAX_STORAGE+1, MAX_STORAGE+1))\n",
    "    PI = np.zeros((MAX_STORAGE+1, MAX_STORAGE+1), dtype=int)\n",
    "\n",
    "    fig = plt.figure(figsize=(22,14))\n",
    "    axes = [fig.add_subplot(2, 3, i+1) for i in range(5)]\n",
    "    ax3d = fig.add_subplot(2,3,6, projection=\"3d\")\n",
    "\n",
    "    show_policy(axes[0], 0, PI)\n",
    "\n",
    "    for k in range(1, 5):   # match Sutton & Barto figure count\n",
    "        evaluate_policy(V, PI)\n",
    "        stop = improve_policy(V, PI)\n",
    "        show_policy(axes[k], k, PI)\n",
    "        if stop: break\n",
    "\n",
    "    show_value(ax3d, V, k)\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "    print(f\"Final output stored as {fname}\")\n",
    "\n",
    "\n",
    "# run full solver\n",
    "run_policy_iteration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90473d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
